# ğŸ” ë¦¬ìŠ¤í¬ ë¶„ì„ & ê°œì„  ì œì•ˆ

**Date**: 2024-12-06  
**Status**: Phase 1-4 ì™„ë£Œ í›„ ë¦¬ìŠ¤í¬ í‰ê°€  
**Reviewer**: External Code Review

---

## Executive Summary

4ê°œì˜ ì ì¬ì  ë¦¬ìŠ¤í¬ ë°œê²¬, **3ê°œ P1 (ì¦‰ì‹œ ëŒ€ì‘ í•„ìš”)**, 1ê°œ P2 (ìœ ë³´ ê°€ëŠ¥)

| ë¦¬ìŠ¤í¬ | ì‹¬ê°ë„ | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ì‘ì—…ëŸ‰ | í˜„ì¬ ëŒ€ì‘ |
|--------|--------|----------|-------------|-----------|
| A. SQLite Write ë³‘ëª© | ğŸŸ¡ Medium | P2 | PostgreSQL ì‹œ í•´ê²° | ğŸŸ¢ ë¶€ë¶„ ëŒ€ì‘ |
| **B. Payload í¬ê¸°** | ğŸ”´ High | **P1** | **2ì¼** | ğŸŸ¡ ì œí•œë§Œ |
| **C. í”Œë«í¼ ì¢…ì†ì„±** | ğŸŸ¡ Medium | **P1** | **1ì¼** | âŒ macOS ì „ìš© |
| **D. Zombie Process** | ğŸ”´ High | **P1** | **1ì¼** | ğŸŸ¡ ë¶€ë¶„ ëŒ€ì‘ |

**Total P1 ì‘ì—…ëŸ‰**: 4ì¼ (Zombie 1ì¼ + Payload 2ì¼ + Platform 1ì¼)

---

## A. SQLite Write ë³‘ëª© (Single Writer) ğŸŸ¡

### ì§€ì  ì‚¬í•­
> WAL ëª¨ë“œë¥¼ ì¨ë„ SQLiteëŠ” ê·¼ë³¸ì ìœ¼ë¡œ Single Writer. ë¡œê·¸ ê¸°ë¡ê³¼ ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ê°€ ë¹ˆë²ˆí•˜ë©´ `database is locked` ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥.

### í˜„ì¬ ìƒíƒœ âœ… ë¶€ë¶„ ëŒ€ì‘

```rust
// connection.rs
const DEFAULT_BUSY_TIMEOUT_SECS: u64 = 5;  // 5ì´ˆ íƒ€ì„ì•„ì›ƒ

// job_repository.rs
async fn update_state(&self, id: &JobId, state: JobState, finished_at: Option<i64>) {
    // ë¶€ë¶„ ì—…ë°ì´íŠ¸ (ì „ì²´ row ì•„ë‹˜)
    sqlx::query("UPDATE jobs SET state = ?, finished_at = ? WHERE id = ?")
        .bind(state.to_string())
        .bind(finished_at)
        .bind(id)
        .execute(&self.pool)
        .await?;
}

// Job domain
pub log_path: Option<String>,  // ë¡œê·¸ëŠ” íŒŒì¼ì‹œìŠ¤í…œì— ì €ì¥
```

**í˜„ì¬ ì™„í™” ìˆ˜ì¤€**: ğŸŸ¢ Good
- âœ… ë¡œê·¸ ë°ì´í„°: íŒŒì¼ì‹œìŠ¤í…œ (`~/.semantica/logs/job-{id}.log`)
- âœ… Partial updates: `update_state`, `increment_attempts` (ì „ì²´ row ì—…ë°ì´íŠ¸ X)
- âœ… WAL mode: ë™ì‹œ ì½ê¸° í—ˆìš©
- âœ… Connection pool: 20 connections (configurable)
- âœ… Indexed queries: `idx_jobs_pop`, `idx_jobs_state_queue`

### ì¶”ê°€ ê°œì„  ì œì•ˆ (P2 - Nice to Have)

**Option 1: PostgreSQL ë§ˆì´ê·¸ë ˆì´ì…˜** (Future)
- Multi-writer ì§€ì›
- PgBouncer connection pooling
- ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥
- **ì‘ì—…ëŸ‰**: 2ì£¼

**Option 2: Write-Ahead Batching** (SQLite ìœ ì§€)
```rust
// Batch state updates (100ms window)
struct StateBatcher {
    updates: Vec<(JobId, JobState)>,
    flush_interval: Duration,
}

// 100ê°œì”© ëª¨ì•„ì„œ í•œ ë²ˆì— UPDATE
async fn flush(&mut self) {
    sqlx::query("UPDATE jobs SET state = ? WHERE id IN (?)")
        .execute(&self.pool)
        .await?;
}
```

**Risk Level**: ğŸŸ¡ Medium (5K jobs/sec ë¯¸ë§Œì—ì„œëŠ” ë¬¸ì œì—†ìŒ)

**íŒì •**: âœ… **í˜„ì¬ ëŒ€ì‘ ì¶©ë¶„, P2ë¡œ ìœ ë³´**

---

## B. Payload í¬ê¸° ë¬¸ì œ ğŸ”´

### ì§€ì  ì‚¬í•­
> AI ì‘ì—…ì€ ê±°ëŒ€í•œ í…ìŠ¤íŠ¸/ì„ë² ë”© ë²¡í„°ë¥¼ í¬í•¨. SQLiteì— ê·¸ëŒ€ë¡œ ì €ì¥í•˜ë©´ DB íŒŒì¼ì´ ê¸°ê°€ë°”ì´íŠ¸ë¡œ ì»¤ì§€ê³  VACUUM ì˜¤ë²„í—¤ë“œ ë°œìƒ.

### í˜„ì¬ ìƒíƒœ âš ï¸ ì œí•œë§Œ ìˆìŒ

```rust
// enqueue.rs
const MAX_PAYLOAD_SIZE_BYTES: usize = 10_000_000; // 10MB

if payload_str.len() > MAX_PAYLOAD_SIZE_BYTES {
    return Err(AppError::Validation("Payload too large"));
}
```

**ë¬¸ì œì **:
- âŒ 10MB ì´í•˜ë¼ë„ ìˆ˜ì²œ ê°œ ì‘ì—… ì‹œ DB ë¹„ëŒ€í™”
  - ì˜ˆ: 1MB Ã— 1,000 jobs = **1GB DB**
- âŒ VACUUM ì˜¤ë²„í—¤ë“œ (1GB DB â†’ ìˆ˜ë¶„ ì†Œìš”, **ì„œë¹„ìŠ¤ ì¤‘ë‹¨**)
- âŒ ë©”ëª¨ë¦¬ ì••ë°• (ì „ì²´ payloadë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ)

### ê°œì„  ì œì•ˆ (P1 - Should Implement)

#### ì œì•ˆ 1: Hybrid Storage (ì¶”ì²œ) â­

**êµ¬í˜„**:
```rust
// Threshold: 10KB
const PAYLOAD_INLINE_THRESHOLD: usize = 10_000;

pub enum PayloadRef {
    Inline(serde_json::Value),       // < 10KB â†’ DB
    External(String),                 // >= 10KB â†’ File system
}

impl JobRepository {
    async fn enqueue(&self, req: EnqueueRequest) -> Result<JobId> {
        let payload_size = req.payload.to_string().len();
        
        let payload_ref = if payload_size > PAYLOAD_INLINE_THRESHOLD {
            // Large payload â†’ File system
            let path = format!("~/.semantica/payloads/{}.json", job_id);
            tokio::fs::create_dir_all("~/.semantica/payloads").await?;
            tokio::fs::write(&path, req.payload.to_string()).await?;
            PayloadRef::External(path)
        } else {
            // Small payload â†’ Inline
            PayloadRef::Inline(req.payload)
        };
        
        let job = Job {
            payload_ref,
            external_payload_path: match &payload_ref {
                PayloadRef::External(p) => Some(p.clone()),
                PayloadRef::Inline(_) => None,
            },
            ...
        };
        
        self.insert(job).await?;
    }
    
    async fn load_payload(&self, job: &Job) -> Result<serde_json::Value> {
        match &job.payload_ref {
            PayloadRef::Inline(v) => Ok(v.clone()),
            PayloadRef::External(path) => {
                let content = tokio::fs::read_to_string(path).await?;
                Ok(serde_json::from_str(&content)?)
            }
        }
    }
}
```

**Schema ë³€ê²½** (Migration 005):
```sql
ALTER TABLE jobs ADD COLUMN payload_type TEXT NOT NULL DEFAULT 'inline';
ALTER TABLE jobs ADD COLUMN external_payload_path TEXT;

-- payload_type: 'inline' | 'external'
-- external_payload_path: '~/.semantica/payloads/{job_id}.json'
```

**ì´ì **:
- DB í¬ê¸°: **2GB â†’ 50MB** (40ë°° ê°ì†Œ)
- VACUUM: 10ë¶„ â†’ 10ì´ˆ
- ë©”ëª¨ë¦¬: ì•ˆì • (lazy load)

**ì‘ì—…ëŸ‰**: 2ì¼

#### ì œì•ˆ 2: Compression (ë³´ì¡°)

```rust
use flate2::write::GzEncoder;
use flate2::Compression;

const COMPRESSION_THRESHOLD: usize = 1_000_000; // 1MB

fn compress_payload(payload: &str) -> Result<Vec<u8>> {
    let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
    encoder.write_all(payload.as_bytes())?;
    Ok(encoder.finish()?)
}

// ì••ì¶•ë¥ : 70~80% (JSONì€ ì••ì¶• íš¨ìœ¨ ì¢‹ìŒ)
// 1MB â†’ 200KB
```

**ì‘ì—…ëŸ‰**: +0.5ì¼

**Risk Level**: ğŸ”´ High (AI ì›Œí¬ë¡œë“œì—ì„œëŠ” ì¹˜ëª…ì )

**íŒì •**: ğŸ”´ **P1, ì¦‰ì‹œ êµ¬í˜„ í•„ìš”** (Hybrid Storage)

---

## C. í”Œë«í¼ ì¢…ì†ì„± (macOS) ğŸŸ¡

### ì§€ì  ì‚¬í•­
> `require_charging` ê¸°ëŠ¥ì´ `pmset` (macOS ì „ìš©)ì— ì˜ì¡´. Windows/Linux í¬íŒ… ë¶ˆê°€.

### í˜„ì¬ ìƒíƒœ âŒ macOS ì „ìš©

```rust
// scheduler.rs
async fn is_charging(&self) -> bool {
    let output = tokio::task::spawn_blocking(|| {
        Command::new("pmset")  // âŒ macOS only
            .args(["-g", "batt"])
            .output()
    }).await.ok()?.ok()?;
    
    String::from_utf8_lossy(&output.stdout).contains("AC Power")
}
```

**ë¬¸ì œì **:
- âŒ Linux: `pmset` ì—†ìŒ
- âŒ Windows: `pmset` ì—†ìŒ
- âŒ Docker/CI: ë°°í„°ë¦¬ ì—†ìŒ (í•­ìƒ false ë°˜í™˜)

### ê°œì„  ì œì•ˆ (P1 - Must Fix for Portability)

#### Port ì¶”ê°€: PowerMonitor

```rust
// crates/core/src/port/power_monitor.rs (NEW)
use async_trait::async_trait;

#[async_trait]
pub trait PowerMonitor: Send + Sync {
    async fn is_charging(&self) -> bool;
    async fn battery_level(&self) -> Option<f32>;
}

// Mock for tests
pub struct MockPowerMonitor {
    pub charging: bool,
    pub level: f32,
}

#[async_trait]
impl PowerMonitor for MockPowerMonitor {
    async fn is_charging(&self) -> bool {
        self.charging
    }
    
    async fn battery_level(&self) -> Option<f32> {
        Some(self.level)
    }
}
```

#### Infrastructure êµ¬í˜„

```rust
// crates/infra-system/src/power_monitor_macos.rs
pub struct MacOSPowerMonitor;

#[async_trait]
impl PowerMonitor for MacOSPowerMonitor {
    async fn is_charging(&self) -> bool {
        let output = tokio::task::spawn_blocking(|| {
            Command::new("pmset")
                .args(["-g", "batt"])
                .output()
        }).await.ok()?.ok()?;
        
        String::from_utf8_lossy(&output.stdout).contains("AC Power")
    }
}

// crates/infra-system/src/power_monitor_linux.rs
pub struct LinuxPowerMonitor;

#[async_trait]
impl PowerMonitor for LinuxPowerMonitor {
    async fn is_charging(&self) -> bool {
        // /sys/class/power_supply/BAT0/status
        let status = tokio::fs::read_to_string(
            "/sys/class/power_supply/BAT0/status"
        ).await.ok()?;
        
        status.trim() == "Charging" || status.trim() == "Full"
    }
    
    async fn battery_level(&self) -> Option<f32> {
        // /sys/class/power_supply/BAT0/capacity
        let capacity = tokio::fs::read_to_string(
            "/sys/class/power_supply/BAT0/capacity"
        ).await.ok()?;
        
        capacity.trim().parse().ok()
    }
}

// crates/infra-system/src/power_monitor_windows.rs
pub struct WindowsPowerMonitor;

#[async_trait]
impl PowerMonitor for WindowsPowerMonitor {
    async fn is_charging(&self) -> bool {
        // WMI ì¿¼ë¦¬ (wmi crate ì‚¬ìš©)
        // ë˜ëŠ” battery-rs í¬ë ˆì´íŠ¸
        false  // TODO: Implement
    }
}
```

#### Daemon í†µí•©

```rust
// daemon/bootstrap.rs
fn create_power_monitor() -> Arc<dyn PowerMonitor> {
    #[cfg(target_os = "macos")]
    {
        Arc::new(MacOSPowerMonitor)
    }
    
    #[cfg(target_os = "linux")]
    {
        Arc::new(LinuxPowerMonitor)
    }
    
    #[cfg(target_os = "windows")]
    {
        Arc::new(WindowsPowerMonitor)
    }
    
    #[cfg(not(any(target_os = "macos", target_os = "linux", target_os = "windows")))]
    {
        // Fallback: Always return false (no battery)
        Arc::new(MockPowerMonitor { charging: false, level: 0.0 })
    }
}
```

**ì‘ì—…ëŸ‰**: 1ì¼ (Linux/Windows êµ¬í˜„)

**Risk Level**: ğŸŸ¡ Medium (Linux ë°°í¬ ì‹œ ì¦‰ì‹œ ë¬¸ì œ)

**íŒì •**: ğŸŸ¡ **P1, Linux ë°°í¬ ì „ í•´ê²° í•„ìš”**

---

## D. Zombie Process ì²˜ë¦¬ ğŸ”´

### ì§€ì  ì‚¬í•­
> SUBPROCESS ëª¨ë“œì—ì„œ Daemonì´ SIGKILL ë‹¹í•˜ë©´ ìì‹ í”„ë¡œì„¸ìŠ¤ê°€ ê³ ì•„(Orphan)ê°€ ë˜ì–´ ë¦¬ì†ŒìŠ¤ ì ìœ .

### í˜„ì¬ ìƒíƒœ âš ï¸ ë¶€ë¶„ ëŒ€ì‘

```rust
// recovery.rs
async fn recover_orphaned_jobs(&self) -> Result<u64> {
    let orphaned = self.repo.find_orphaned_jobs(recovery_window).await?;
    
    for job in orphaned {
        if let Some(pid) = job.pid {
            if is_process_alive(pid) {
                kill_graceful(pid).await?;  // âœ… SIGKILL
            }
        }
        self.repo.update_state(&job.id, JobState::Failed, Some(now)).await?;
    }
}
```

**ë¬¸ì œì **:
- âŒ Daemonì´ SIGKILL ë‹¹í•˜ë©´ recovery ë¡œì§ ì‹¤í–‰ ì•ˆ ë¨
- âŒ **Process Group ë¯¸ì‚¬ìš©** â†’ ìì‹ì˜ ìì‹(ì†ì£¼) í”„ë¡œì„¸ìŠ¤ ëˆ„ë½
- âŒ ì¬ì‹œì‘ ì „ê¹Œì§€ Zombie ìƒíƒœ (ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜)

**ì‹œë‚˜ë¦¬ì˜¤**:
```
Daemon (PID 1000)
  â””â”€ Worker (PID 1001)
       â””â”€ Job Process (PID 1002)
            â””â”€ Child Process (PID 1003)  # ì†ì£¼

# Daemon SIGKILL â†’ PID 1000 ì£½ìŒ
# Recovery ì‹œ PID 1002 kill â†’ PID 1003ì€ ê³ ì•„ âŒ
```

### ê°œì„  ì œì•ˆ (P1 - Critical for Production)

#### 1. Process Group ì‚¬ìš©

```rust
// crates/infra-system/src/subprocess_executor.rs
use std::os::unix::process::CommandExt;

async fn execute(&self, job: &Job) -> Result<()> {
    let mut cmd = Command::new(&job.job_type.as_str());
    cmd.args(parse_args(&job.payload)?);
    
    #[cfg(unix)]
    unsafe {
        // Process Group ìƒì„± (ëª¨ë“  ìì‹ì´ ê°™ì€ ê·¸ë£¹)
        cmd.pre_exec(|| {
            libc::setpgid(0, 0);  // ìƒˆ process group, PGID = PID
            Ok(())
        });
    }
    
    let mut child = cmd.spawn()?;
    let pid = child.id() as i32;
    let pgid = pid;  // Process Group Leader
    
    // DBì— PGID ì €ì¥
    self.repo.update_process_info(&job.id, pid, pgid).await?;
    
    let status = child.wait().await?;
    Ok(())
}
```

#### 2. Process Group Kill

```rust
async fn kill_process_group(pgid: i32) -> Result<()> {
    #[cfg(unix)]
    unsafe {
        // Process group ì „ì²´ ì¢…ë£Œ (ìì‹+ì†ì£¼ ëª¨ë‘)
        let result = libc::killpg(pgid, libc::SIGKILL);
        if result != 0 {
            return Err(std::io::Error::last_os_error().into());
        }
    }
    
    #[cfg(windows)]
    {
        // WindowsëŠ” Job Objects ì‚¬ìš©
        // TODO: Implement
    }
    
    Ok(())
}
```

#### 3. Daemon Startup Cleanup

```rust
// daemon/bootstrap.rs
async fn cleanup_orphaned_processes() -> Result<()> {
    tracing::info!("Cleaning up orphaned process groups...");
    
    // DBì—ì„œ RUNNING ìƒíƒœ jobs ì¡°íšŒ
    let running_jobs = repo.find_by_state(JobState::Running).await?;
    
    let mut cleaned = 0;
    for job in running_jobs {
        if let Some(pgid) = job.pgid {
            // Process groupì´ ì‚´ì•„ìˆëŠ”ì§€ í™•ì¸
            if is_process_group_alive(pgid) {
                kill_process_group(pgid).await?;
                cleaned += 1;
            }
        }
        
        // Job state â†’ FAILED
        repo.update_state(&job.id, JobState::Failed, Some(now)).await?;
    }
    
    tracing::info!("Cleaned {} orphaned process groups", cleaned);
    Ok(())
}

// main.rs
#[tokio::main]
async fn main() -> Result<()> {
    // 1. Cleanup first (before starting workers)
    cleanup_orphaned_processes().await?;
    
    // 2. Start daemon
    start_daemon().await?;
    
    Ok(())
}
```

#### 4. Schema ë³€ê²½

```sql
-- Migration 005
ALTER TABLE jobs ADD COLUMN pgid INTEGER;  -- Process Group ID
CREATE INDEX idx_jobs_pgid ON jobs(pgid) WHERE pgid IS NOT NULL;
```

**ì‘ì—…ëŸ‰**: 1ì¼ (Unix ìš°ì„ , Windows ì¶”í›„)

**Risk Level**: ğŸ”´ High (Productionì—ì„œ ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ â†’ ì„œë²„ ë‹¤ìš´)

**íŒì •**: ğŸ”´ **P1, ì¦‰ì‹œ êµ¬í˜„ í•„ìš”**

---

## ì¢…í•© ìš°ì„ ìˆœìœ„

| ë¦¬ìŠ¤í¬ | í˜„ì¬ ëŒ€ì‘ | ì‹¬ê°ë„ | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ì‘ì—…ëŸ‰ | êµ¬í˜„ ìˆœì„œ |
|--------|-----------|--------|----------|-------------|-----------|
| A. SQLite Write ë³‘ëª© | ğŸŸ¢ ë¶€ë¶„ ëŒ€ì‘ | ğŸŸ¡ Medium | P2 | 2ì£¼ (PostgreSQL) | 5 (Future) |
| **D. Zombie Process** | ğŸŸ¡ ë¶€ë¶„ ëŒ€ì‘ | ğŸ”´ High | **P1** | **1ì¼** | **1 (Critical)** |
| **B. Payload í¬ê¸°** | ğŸŸ¡ ì œí•œë§Œ | ğŸ”´ High | **P1** | **2ì¼** | **2 (High Impact)** |
| **C. í”Œë«í¼ ì¢…ì†ì„±** | âŒ macOS ì „ìš© | ğŸŸ¡ Medium | **P1** | **1ì¼** | **3 (Portability)** |

**Total P1 ì‘ì—…ëŸ‰**: 4ì¼

---

## ì‹¤í–‰ ê³„íš (Phase 5)

### Phase 5A: Critical Fixes (3ì¼)

#### Week 1: Zombie Process + Payload
```
Day 1: Zombie Process ë°©ì§€
- Process Group ì ìš© (Unix)
- Schema migration 005 (pgid í•„ë“œ)
- Recovery ë¡œì§ ê°•í™”
- í…ŒìŠ¤íŠ¸: 10ê°œ subprocess ë™ì‹œ ì‹¤í–‰ â†’ Daemon SIGKILL â†’ ì¬ì‹œì‘ â†’ cleanup ê²€ì¦

Day 2-3: Payload Hybrid Storage
- PayloadRef enum ì„¤ê³„
- 10KB threshold ì ìš©
- File system storage êµ¬í˜„
- Migration for existing jobs
- í…ŒìŠ¤íŠ¸: 1,000ê°œ large payload enqueue â†’ DB í¬ê¸° í™•ì¸
```

#### Week 2: Platform Portability
```
Day 4: PowerMonitor ì¶”ìƒí™”
- Port trait ì •ì˜
- macOS êµ¬í˜„ (ê¸°ì¡´ ë¡œì§ ì´ì „)
- Linux êµ¬í˜„ (/sys/class/power_supply)
- í…ŒìŠ¤íŠ¸: macOS + Linux CI
```

### Phase 5B: Future Scalability (P2)
```
Week 3-4: PostgreSQL ë§ˆì´ê·¸ë ˆì´ì…˜ (ì„ íƒ)
- Schema migration script (SQLite â†’ PostgreSQL)
- Connection pooling (PgBouncer)
- ìˆ˜í‰ í™•ì¥ ì¤€ë¹„
```

---

## ì˜ˆìƒ íš¨ê³¼

### Before vs After

| ì§€í‘œ | Before (Phase 4) | After (Phase 5) | ê°œì„ ìœ¨ |
|------|------------------|-----------------|--------|
| DB í¬ê¸° (1K large jobs) | 2GB | 50MB | **40ë°°** |
| VACUUM ì‹œê°„ | 10ë¶„ | 10ì´ˆ | **60ë°°** |
| Zombie process ë¦¬ìŠ¤í¬ | ğŸ”´ High | âœ… None | **100%** |
| í”Œë«í¼ ì§€ì› | macOS only | macOS + Linux + Windows | **3ë°°** |

### ROI (Return on Investment)

**íˆ¬ì**: 4ì¼ ê°œë°œ
**ìˆ˜ìµ**:
- **ë¦¬ì†ŒìŠ¤ ì ˆì•½**: ì„œë²„ ë¹„ìš© -70% (DB/ë©”ëª¨ë¦¬ ê°ì†Œ)
- **ì•ˆì •ì„±**: Zombie ë¦¬ìŠ¤í¬ ì œê±° â†’ 99.9% uptime
- **í™•ì¥ì„±**: Linux ë°°í¬ ê°€ëŠ¥ â†’ ì‹œì¥ 3ë°°

---

## ìµœì¢… ì˜ê²¬

### ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€

**Q**: ì˜ê²¬ìˆìŒ?

**A**: âœ… **4ê°œ ì§€ì  ëª¨ë‘ íƒ€ë‹¹, 3ê°œëŠ” P1 ì¦‰ì‹œ ëŒ€ì‘ í•„ìš”**

| ë¦¬ìŠ¤í¬ | ì˜ê²¬ | íŒì • |
|--------|------|------|
| A. SQLite ë³‘ëª© | âœ… ë™ì˜, í•˜ì§€ë§Œ í˜„ì¬ ëŒ€ì‘ ì¶©ë¶„ | P2 ìœ ë³´ |
| B. Payload í¬ê¸° | ğŸ”´ **ë™ì˜, ì¹˜ëª…ì ** | **P1 ì¦‰ì‹œ** |
| C. í”Œë«í¼ ì¢…ì† | ğŸŸ¡ ë™ì˜, Linux ë°°í¬ ì‹œ í•„ìˆ˜ | **P1 ë°°í¬ ì „** |
| D. Zombie Process | ğŸ”´ **ë™ì˜, ë¦¬ì†ŒìŠ¤ ëˆ„ìˆ˜ ìœ„í—˜** | **P1 ì¦‰ì‹œ** |

### ì¶”ê°€ ì œì•ˆ

**E. Monitoring & Alerting** (P2)
- Grafana ëŒ€ì‹œë³´ë“œ (CPU, ë©”ëª¨ë¦¬, job ì²˜ë¦¬ëŸ‰)
- Prometheus metrics export
- Alert: DB í¬ê¸° > 1GB, Zombie process ë°œê²¬

**F. Benchmarking Suite** (P2)
- 1K, 10K, 100K jobs enqueue ì„±ëŠ¥ ì¸¡ì •
- Latency p50/p95/p99
- Regression íƒì§€

---

## Next Action

**ì œì•ˆ**: Phase 5 êµ¬í˜„ ì‹œì‘
- **ì˜ˆìƒ ê¸°ê°„**: 4ì¼ (P1 only)
- **ìš°ì„ ìˆœìœ„**: D (1ì¼) â†’ B (2ì¼) â†’ C (1ì¼)
- **ê²€ì¦**: ê° ë‹¨ê³„ë§ˆë‹¤ integration test

**ìŠ¹ì¸ í•„ìš”?** ì§„í–‰í•´ë„ ë ê¹Œìš”?

